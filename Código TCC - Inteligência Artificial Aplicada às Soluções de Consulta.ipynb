{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando bibliotecas para webscrapping e manipulações iniciais\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "#Importando bibliotecas para tratamento de dados e exploração das informações\n",
    "import nltk, os, spacy\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import RSLPStemmer\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "#importando bibliotecas de machine learning\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import time\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo a primeira página\n",
    "df= pd.read_html('http://normas.receita.fazenda.gov.br/sijut2consulta/consulta.action?facetsExistentes=tipoAtoFacet%2CsiglaOrgaoFacet&orgaosSelecionados=&tiposAtosSelecionados=72&lblTiposAtosSelecionados=SC&ordemColuna=&ordemDirecao=&tipoConsulta=formulario&tipoAtoFacet=Solu%C3%A7%C3%A3o+de+Consulta&siglaOrgaoFacet=Cosit&anoAtoFacet=&termoBusca=&numero_ato=&tipoData=2&dt_inicio=&dt_fim=&ano_ato=&optOrdem=relevancia&p=1',\n",
    "                 attrs = {'id': \"tabelaAtos\"}, \n",
    "                 skiprows = 1, \n",
    "                 header = 0, \n",
    "                 flavor = 'bs4')[0][:-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo as demais páginas e fazendo append no DF criado\n",
    "Base_url ='http://normas.receita.fazenda.gov.br/sijut2consulta/consulta.action?facetsExistentes=tipoAtoFacet%2CsiglaOrgaoFacet&orgaosSelecionados=&tiposAtosSelecionados=72&lblTiposAtosSelecionados=SC&ordemColuna=&ordemDirecao=&tipoConsulta=formulario&tipoAtoFacet=Solu%C3%A7%C3%A3o+de+Consulta&siglaOrgaoFacet=Cosit&anoAtoFacet=&termoBusca=&numero_ato=&tipoData=2&dt_inicio=&dt_fim=&ano_ato=&optOrdem=relevancia&p={}'\n",
    "for pg in range(2,48):\n",
    "    df2 = pd.read_html(Base_url.format(pg), \n",
    "                       attrs = {'id': \"tabelaAtos\"}, \n",
    "                       skiprows = 1, \n",
    "                       header = 0, \n",
    "                       flavor = 'bs4')[0][:-2]\n",
    "    df = df.append(df2, ignore_index = True, sort= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visualizando o Data Frame\n",
    "df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cópia de backup\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('dataframe_baixado_backup', sep = ';', encoding = 'utf-8-sig') \n",
    "#Último download. Utilizar o arquivo caso tenha que rodar código novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataframe_baixado_backup', sep = ';', encoding = 'utf-8-sig') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_emen = df[\"Ementa\"]\n",
    "lista_emen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colocando as ementas em uma lista e convertendo para uppercase\n",
    "lista_emen = df[\"Ementa\"]\n",
    "lista_ementa = []\n",
    "\n",
    "for i in lista_emen:\n",
    "    a= i.upper()\n",
    "    lista_ementa.append(a)\n",
    "\n",
    "lista_ementa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_ementa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a coluna assuntos - Classe\n",
    "\n",
    "a1 = 'Assunto: Classificação de Mercadorias'\n",
    "a2 = 'Assunto: Simples Nacional'\n",
    "a3 = 'Assunto: CONTRIBUIÇÃO PARA O FIN'\n",
    "a4 = 'Assunto: Imposto sobre a Renda de Pessoa Jurídica'\n",
    "a5 = 'Assunto: Obrigações Acessórias'\n",
    "a6 = 'Assunto: IMPOSTO SOBRE A RENDA RETIDO NA FONTE'\n",
    "a7 = 'Assunto: Contribuição para o PIS/Pasep'\n",
    "a8 = 'Assunto: Imposto sobre Operações de Crédito, Câmbio e Seguros ou relativas a Títulos ou Valores Mobiliários - IOF'\n",
    "a9 = 'Assunto: Contribuições Sociais Previdenciárias'\n",
    "a10 = 'Assunto: Imposto sobre Produtos Industrializados'\n",
    "a11 = 'Assunto: Imposto sobre a Importação'\n",
    "a12 = 'Assunto: REGIMES ADUANEIROS'\n",
    "a13 = 'Assunto: NORMAS GERAIS DE DIREITO TRIBUTÁRIO'\n",
    "a14 = 'Assunto: NORMAS DE ADMINISTRAÇÃO TRIBUTÁRIA'\n",
    "a15 = 'Assunto: Imposto sobre Operações de Crédito, Câmbio e Seguros ou relativas a Títulos ou Valores Mobiliários – IOF'\n",
    "a16 = 'Assunto: Imposto sobre a Renda de Pessoa Física'\n",
    "a17 = 'Assunto: Normas Gerais'\n",
    "a18 = 'Assunto: Normas de Administração Tributária'\n",
    "a19 = 'Assunto: PESSOA JURÍDICA'\n",
    "a20 = 'Assunto: PESSOA FÍSICA'\n",
    "a21 = 'Assunto: OBRIGAÇÕES ACESSÓRIAS'\n",
    "a22 = 'Assunto: IMPOSTO SOBRE A RENDA RETIDO NA FONTE'\n",
    "a23 = 'Assunto: CONTRIBUIÇÃO DE INTERVENÇÃO NO DOMÍNIO ECONÔMICO'\n",
    "a24 = 'Assunto: IMPOSTO SOBRE A PROPRIEDADE TERRITORIAL RURAL'\n",
    "a25 = 'Assunto: SEGURIDADE SOCIAL'\n",
    "a26 = 'Retificação'\n",
    "a27 = 'Assunto: CONTRIBUIÇÃO SOCIAL SOBRE O LUCRO LÍQUIDO'\n",
    "a28 = 'Assunto: CONTRIBUIÇÕES PREVIDENCIÁRIAS'\n",
    "a29 = 'Assunto: PROCESSO ADMINISTRATIVO FISCAL'\n",
    "a30 = 'Assunto: Contribuições para a Previdência Social'\n",
    "a31 = 'Assunto: REGIME ESPECIAL DE REGULARIZAÇÃO CAMBIAL E TRIBUTÁRIA'\n",
    "a32 = 'Assunto: Outros Tributos ou Contribuições'\n",
    "a33 = 'Assunto: REGIME ESPECIAL DE REGULARIZAÇÃO'\n",
    "a34 = 'Assunto: DIREITOS ANTIDUMPING'\n",
    "a35 = 'Assunto: IMPOSTO DE RENDA RETIDO NA FONTE'\n",
    "a36 = 'ASSUNTO: IMPOSTO SOBRE A RENDA DA PESSOA JURÍDICA '\n",
    "a37 = 'ASSUNTO: Classificação de Mercadorias'\n",
    "a38 = 'Assunto:CONTRIBUIÇÃO PARA O FIN'\n",
    "a39 = 'ASSUNTO:IMPOSTO SOBRE A RENDA DE PESSOA JURÍDICA'\n",
    "a40 = 'ASSUNTO:Contribuição para o PIS/Pasep'\n",
    "a41 = 'Assunto:Imposto sobre a Renda Retido na Fonte'\n",
    "a42 = 'Contribuições Sociais e Previdenciárias'\n",
    "a43 = 'Normas Gerais de Direito Tributário'\n",
    "a44 = 'IOF'\n",
    "\n",
    "classe_assunto=[]\n",
    "\n",
    "\n",
    "for i in lista_ementa:\n",
    "    if i.startswith(a1.upper()) == True:\n",
    "        classe_assunto.append(a1)\n",
    "    elif i.startswith(a2.upper())==True:\n",
    "        classe_assunto.append(a2)\n",
    "    elif i.startswith(a3.upper())==True:\n",
    "         classe_assunto.append(a42)\n",
    "    elif i.startswith(a4.upper())==True:\n",
    "        classe_assunto.append(a4)\n",
    "    elif i.startswith(a5.upper())==True:\n",
    "        classe_assunto.append(a5)\n",
    "    elif i.startswith(a6.upper())==True:\n",
    "        classe_assunto.append(a6)\n",
    "    elif i.startswith(a7.upper())== True:\n",
    "         classe_assunto.append(a42)\n",
    "    elif i.startswith(a8.upper())== True:\n",
    "        classe_assunto.append(a44)\n",
    "    elif i.startswith(a9.upper())== True:\n",
    "        classe_assunto.append(a42)\n",
    "    elif i.startswith(a10.upper())== True:\n",
    "        classe_assunto.append(a10)\n",
    "    elif i.startswith(a11.upper())== True:\n",
    "         classe_assunto.append(a11)\n",
    "    elif i.startswith(a12.upper())== True:\n",
    "        classe_assunto.append(a12)\n",
    "    elif i.startswith(a13.upper())== True:\n",
    "        classe_assunto.append(a43)\n",
    "    elif i.startswith(a14.upper())== True:\n",
    "        classe_assunto.append(a14)\n",
    "    elif i.startswith(a15.upper())== True:\n",
    "        classe_assunto.append(a44)\n",
    "    elif i.startswith(a16.upper())== True:\n",
    "        classe_assunto.append(a16)\n",
    "    elif i.startswith(a17.upper())== True:\n",
    "        classe_assunto.append(a43)\n",
    "    elif i.startswith(a18.upper())== True:\n",
    "        classe_assunto.append(a18)\n",
    "    elif i.startswith(a19.upper())== True:\n",
    "        classe_assunto.append(a4)\n",
    "    elif i.startswith(a20.upper())== True:\n",
    "        classe_assunto.append(a16)\n",
    "    elif i.startswith(a21.upper())== True:\n",
    "        classe_assunto.append(a21)\n",
    "    elif i.startswith(a22.upper())== True:\n",
    "        classe_assunto.append(a6)\n",
    "    elif i.startswith(a23.upper())== True:\n",
    "        classe_assunto.append(a23)\n",
    "    elif i.startswith(a24.upper())== True:\n",
    "        classe_assunto.append(a24)\n",
    "    elif i.startswith(a25.upper())== True:\n",
    "        classe_assunto.append(a42)      \n",
    "    elif i.startswith(a26.upper())== True:\n",
    "        classe_assunto.append(a26)\n",
    "    elif i.startswith(a27.upper())== True:\n",
    "        classe_assunto.append(a42)\n",
    "    elif i.startswith(a28.upper())== True:\n",
    "        classe_assunto.append(a42)\n",
    "    elif i.startswith(a29.upper())== True:\n",
    "        classe_assunto.append(a29)      \n",
    "    elif i.startswith(a30.upper())== True:\n",
    "        classe_assunto.append(a42)\n",
    "    elif i.startswith(a31.upper())== True:\n",
    "        classe_assunto.append(a31)\n",
    "    elif i.startswith(a32.upper())== True:\n",
    "        classe_assunto.append(a32)\n",
    "    elif i.startswith(a33.upper())== True:\n",
    "        classe_assunto.append(a33)\n",
    "    elif i.startswith(a34.upper())== True:\n",
    "        classe_assunto.append(a34)\n",
    "    elif i.startswith(a35.upper())== True:\n",
    "        classe_assunto.append(a6)\n",
    "    elif i.startswith(a36.upper())== True:\n",
    "        classe_assunto.append(a4)\n",
    "    elif i.startswith(a37.upper())== True:\n",
    "        classe_assunto.append(a1)\n",
    "    elif i.startswith(a38.upper())==True:\n",
    "         classe_assunto.append(a42)\n",
    "    elif i.startswith(a39.upper())==True:\n",
    "         classe_assunto.append(a4)\n",
    "    elif i.startswith(a40.upper())==True:\n",
    "         classe_assunto.append(a42)\n",
    "    elif i.startswith(a41.upper())==True:\n",
    "        classe_assunto.append(a6)\n",
    "    else:\n",
    "        classe_assunto.append(\"Não encontrado\")\n",
    "\n",
    "classe_assunto.count(\"Não encontrado\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe_assunto\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluindo a parte \"assunto\" do assunto\n",
    "\n",
    "B1 = \"ASSUNTO: \"\n",
    "x = classe_assunto\n",
    "y = []\n",
    "assunto_organizado = []\n",
    "\n",
    "for i in x:\n",
    "    h = i.upper()\n",
    "    y.append(h)\n",
    "\n",
    "for i in y:\n",
    "    j = i.replace(B1.upper(), \"\") \n",
    "    assunto_organizado.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluindo os assuntos da ementa\n",
    "\n",
    "a1 = 'Assunto: Classificação de Mercadorias'\n",
    "a2 = 'Assunto: Simples Nacional'\n",
    "a3 = 'Assunto: CONTRIBUIÇÃO PARA O FIN'\n",
    "a4 = 'Assunto: Imposto sobre a Renda de Pessoa Jurídica'\n",
    "a5 = 'Assunto: Obrigações Acessórias'\n",
    "a6 = 'Assunto: IMPOSTO SOBRE A RENDA RETIDO NA FONTE'\n",
    "a7 = 'Assunto: Contribuição para o PIS/Pasep'\n",
    "a8 = 'Assunto: Imposto sobre Operações de Crédito, Câmbio e Seguros ou relativas a Títulos ou Valores Mobiliários - IOF'\n",
    "a9 = 'Assunto: Contribuições Sociais Previdenciárias'\n",
    "a10 = 'Assunto: Imposto sobre Produtos Industrializados'\n",
    "a11 = 'Assunto: Imposto sobre a Importação'\n",
    "a12 = 'Assunto: REGIMES ADUANEIROS'\n",
    "a13 = 'Assunto: NORMAS GERAIS DE DIREITO TRIBUTÁRIO'\n",
    "a14 = 'Assunto: NORMAS DE ADMINISTRAÇÃO TRIBUTÁRIA'\n",
    "a15 = 'Assunto: Imposto sobre Operações de Crédito, Câmbio e Seguros ou relativas a Títulos ou Valores Mobiliários – IOF'\n",
    "a16 = 'Assunto: Imposto sobre a Renda de Pessoa Física'\n",
    "a17 = 'Assunto: Normas Gerais'\n",
    "a18 = 'Assunto: Normas de Administração Tributária'\n",
    "a19 = 'Assunto: PESSOA JURÍDICA'\n",
    "a20 = 'Assunto: PESSOA FÍSICA'\n",
    "a21 = 'Assunto: OBRIGAÇÕES ACESSÓRIAS'\n",
    "a22 = 'Assunto: IMPOSTO SOBRE A RENDA RETIDO NA FONTE'\n",
    "a23 = 'Assunto: CONTRIBUIÇÃO DE INTERVENÇÃO NO DOMÍNIO ECONÔMICO'\n",
    "a24 = 'Assunto: IMPOSTO SOBRE A PROPRIEDADE TERRITORIAL RURAL'\n",
    "a25 = 'Assunto: SEGURIDADE SOCIAL'\n",
    "a26 = 'Retificação'\n",
    "a27 = 'Assunto: CONTRIBUIÇÃO SOCIAL SOBRE O LUCRO LÍQUIDO'\n",
    "a28 = 'Assunto: CONTRIBUIÇÕES PREVIDENCIÁRIAS'\n",
    "a29 = 'Assunto: PROCESSO ADMINISTRATIVO FISCAL'\n",
    "a30 = 'Assunto: Contribuições para a Previdência Social'\n",
    "a31 = 'Assunto: REGIME ESPECIAL DE REGULARIZAÇÃO CAMBIAL E TRIBUTÁRIA'\n",
    "a32 = 'Assunto: Outros Tributos ou Contribuições'\n",
    "a33 = 'Assunto: REGIME ESPECIAL DE REGULARIZAÇÃO'\n",
    "a34 = 'Assunto: DIREITOS ANTIDUMPING'\n",
    "a35 = 'Assunto: IMPOSTO DE RENDA RETIDO NA FONTE'\n",
    "a36 = 'ASSUNTO: IMPOSTO SOBRE A RENDA DA PESSOA JURÍDICA '\n",
    "a37 = 'ASSUNTO: Classificação de Mercadorias'\n",
    "a38 = 'Assunto:CONTRIBUIÇÃO PARA O FIN'\n",
    "a39 = 'ASSUNTO:IMPOSTO SOBRE A RENDA DE PESSOA JURÍDICA'\n",
    "a40 = 'ASSUNTO:Contribuição para o PIS/Pasep'\n",
    "a41 = 'Assunto:Imposto sobre a Renda Retido na Fonte'\n",
    "\n",
    "ementa_organizada=[]\n",
    "\n",
    "\n",
    "for i in lista_ementa:\n",
    "    if i.startswith(a1.upper()) == True:\n",
    "        a = i.replace(a1.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a2.upper())==True:\n",
    "        a = i.replace(a2.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a3.upper())==True:\n",
    "        a = i.replace(a3.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a4.upper())==True:\n",
    "        a = i.replace(a4.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a5.upper())==True:\n",
    "        a = i.replace(a5.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a6.upper())==True:\n",
    "        a = i.replace(a6.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a7.upper())== True:\n",
    "        a = i.replace(a7.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a8.upper())== True:\n",
    "        a = i.replace(a8.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a9.upper())== True:\n",
    "        a = i.replace(a9.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a10.upper())== True:\n",
    "        a = i.replace(a10.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a11.upper())== True:\n",
    "        a = i.replace(a11.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a12.upper())== True:\n",
    "        a = i.replace(a12.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a13.upper())== True:\n",
    "        a = i.replace(a13.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a14.upper())== True:\n",
    "        a = i.replace(a14.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a15.upper())== True:\n",
    "        a = i.replace(a15.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a16.upper())== True:\n",
    "        a = i.replace(a16.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a17.upper())== True:\n",
    "        a = i.replace(a17.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a18.upper())== True:\n",
    "        a = i.replace(a18.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a19.upper())== True:\n",
    "        a = i.replace(a19.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a20.upper())== True:\n",
    "        a = i.replace(a20.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a21.upper())== True:\n",
    "        a = i.replace(a21.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a22.upper())== True:\n",
    "        a = i.replace(a22.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a23.upper())== True:\n",
    "        a = i.replace(a23.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a24.upper())== True:\n",
    "        a = i.replace(a24.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a25.upper())== True:\n",
    "        a = i.replace(a25.upper(), \"\") \n",
    "        ementa_organizada.append(a)      \n",
    "    elif i.startswith(a26.upper())== True:\n",
    "        a = i.replace(a26.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a27.upper())== True:\n",
    "        a = i.replace(a27.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a28.upper())== True:\n",
    "        a = i.replace(a28.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a29.upper())== True:\n",
    "        a = i.replace(a29.upper(), \"\") \n",
    "        ementa_organizada.append(a)      \n",
    "    elif i.startswith(a30.upper())== True:\n",
    "        a = i.replace(a30.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a31.upper())== True:\n",
    "        a = i.replace(a31.upper(), \"\")  \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a32.upper())== True:\n",
    "        a = i.replace(a32.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a33.upper())== True:\n",
    "        a = i.replace(a33.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a34.upper())== True:\n",
    "        a = i.replace(a34.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a35.upper())== True:\n",
    "        a = i.replace(a35.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a36.upper())== True:\n",
    "        a = i.replace(a36.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a37.upper())== True:\n",
    "        a = i.replace(a37.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a38.upper())==True:\n",
    "        a = i.replace(a38.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a39.upper())==True:\n",
    "        a = i.replace(a39.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a40.upper())==True:\n",
    "        a = i.replace(a40.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    elif i.startswith(a41.upper())==True:\n",
    "        a = i.replace(a41.upper(), \"\") \n",
    "        ementa_organizada.append(a)\n",
    "    else:\n",
    "        ementa_organizada.append(\"Não encontrado\")\n",
    "        \n",
    "ementa_organizada.count(\"Não encontrado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ementa_organizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assunto_organizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atualizando o dataset com as colunas já formatadas\n",
    "\n",
    "df[\"Ementa\"] = ementa_organizada\n",
    "df[\"Assunto\"] = assunto_organizado\n",
    "df = df.drop(columns=['Unnamed: 5'])\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo backup do df em csv\n",
    "df.to_csv(\"df3-backup.csv\", sep = ';', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construindo a Pivot Table1\n",
    "table1 = pd.pivot_table(df, values='Nº do ato', index=['Assunto'], aggfunc='count' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query('assunto != [\"OUTROS TRIBUTOS OU CONTRIBUIÇÕES\",\"DIREITOS ANTIDUMPING\",\"REGIME ESPECIAL DE REGULARIZAÇÃO CAMBIAL E TRIBUTÁRIA\",\"RETIFICAÇÃO\", \"PROCESSO ADMINISTRATIVO FISCAL\", \"NORMAS GERAIS\", \"IMPOSTO SOBRE A PROPRIEDADE TERRITORIAL RURAL\",\"NÃO ENCONTRADO\" ]') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construindo a Pivot Table2\n",
    "table2 = pd.pivot_table(df, values= 'ementa', index=['assunto'], aggfunc='count' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table2)\n",
    "print('-------------------------------------------')\n",
    "print(\"Quantidade de classes: \" + str(len(table2)))\n",
    "print(\"Quantidade de linhas no dataset: \" + str(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_blackup.csv', sep = ',', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('df_blackup.csv', sep = ',', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'ementa': df['Ementa'], 'assunto': df['Assunto']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.astype({'ementa': 'string',\n",
    "           'assunto': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEGUNDA LIMPEZA\n",
    "def padronizando(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"CÓDIGO\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"EMENTA\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"ANCIAMENTO DA SEGURIDADE SOCIAL\", \"\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = padronizando(df,'ementa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'ementa': df['Ementa'], 'assunto': df['Assunto']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unindo as stopwords de NLTK e SPACY\n",
    "stopwordsextras =[]\n",
    "stopwordsmantidas=[]\n",
    "stopwordsnltk = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwordspacy = [x for x in STOP_WORDS]\n",
    "stopwordsextra = stopwordsnltk + [x for x in stopwordspacy if x not in stopwordsnltk] + [x for x in stopwordsextras if x not in stopwordsnltk]\n",
    "stopwords = [x for x in stopwordsextra if x not in stopwordsmantidas]\n",
    "\n",
    "\n",
    "print(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.remove('não')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pré-Processamento do texto\n",
    "\n",
    "#retirando acentos\n",
    "ementa3_clean = [unicodedata.normalize('NFKD', s).encode('ISO-8859-1', 'ignore').decode('ISO-8859-1') for s in df['ementa']]\n",
    "\n",
    "#retirando todos os caracteres especiais ->[re.sub('[^\\w\\d\\s]',r'\n",
    "ementa3_clean2 = [re.sub(r'[^a-zA-z]',r' ', doc).lower() for doc in ementa3_clean ]\n",
    "df['ementa_processada'] = ementa3_clean2\n",
    "\n",
    "\n",
    "#primeiro tokeniza, depois retirando as sopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"ementa_processada\"].apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "#Removendo as stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return[token for token in tokens if token not in stopwords]\n",
    "corpus_stops = [' '.join(remove_stopwords(doc)) for doc in df['tokens']]\n",
    "\n",
    "\n",
    "#Tokenizando novamente a lista limpa das stop words\n",
    "df['ementa_processada2'] = corpus_stops\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens2\"] = df[\"ementa_processada2\"].apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "#Realizando o stemming depois de retirada as stopwords\n",
    "def Stemming(sentence):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return phrase\n",
    "\n",
    "a = df[\"tokens2\"]\n",
    "ementa_processada3= []\n",
    "\n",
    "for i in a:\n",
    "    ementa_processada3.append(Stemming(i))\n",
    "\n",
    "#colocando com stemming no df\n",
    "df['ementa_processada3'] = ementa_processada3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pré-Processamento do texto2\n",
    "\n",
    "\n",
    "#primeiro tokeniza, para depois retirar as sopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"ementa\"].apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "#Removendo as stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return[token for token in tokens if token not in stopwords]\n",
    "corpus_stops = [' '.join(remove_stopwords(doc)) for doc in df['tokens']]\n",
    "\n",
    "#retirando acentos\n",
    "df['ementa_limpa']= corpus_stops\n",
    "ementa3_clean = [unicodedata.normalize('NFKD', s).encode('ISO-8859-1', 'ignore').decode('ISO-8859-1') for s in df['ementa_limpa']]\n",
    "\n",
    "#retirando todos os caracteres especiais ->[re.sub('[^\\w\\d\\s]',r'\n",
    "ementa_limpa2 = [re.sub(r'[^a-zA-z]',r' ', doc).lower() for doc in ementa3_clean ]\n",
    "df['ementa_processada'] = ementa_limpa2\n",
    "\n",
    "#Tokenizando novamente a lista limpa das stop words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens2\"] = df[\"ementa_processada\"].apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "#Realizando o stemming depois de retirada as stopwords\n",
    "def Stemming(sentence):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return phrase\n",
    "\n",
    "a = df[\"tokens2\"]\n",
    "ementa_processada2= []\n",
    "\n",
    "for i in a:\n",
    "    ementa_processada2.append(Stemming(i))\n",
    "\n",
    "#colocando com stemming no df\n",
    "df['ementa_processada2'] = ementa_processada2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destokenizando\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "ementa_processada3 = []\n",
    "\n",
    "for i in ementa_processada2:\n",
    "    ementa_processada3.append(TreebankWordDetokenizer().detokenize(i))\n",
    "df['ementa_processada3'] = ementa_processada3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primeira comparação\n",
    "print('Tamanho do corpus sem processar: ' + str(len(df['ementa'][1])))\n",
    "print('\\nExemplo:')\n",
    "print('\\n' + df['ementa'][1])\n",
    "print('\\nTamanho do corpus processado: ' + str(len(df['ementa_processada3'][1])))\n",
    "print('\\nExemplo:')\n",
    "print('\\n'+df['ementa_processada3'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando o df_limpo\n",
    "\n",
    "df_limpo = pd.DataFrame(data={'ementa': df['ementa'],\n",
    "                              'ementa_processada': df['ementa_processada3'],\n",
    "                              'assunto': df['assunto']})\n",
    "df_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo= df_limpo.astype({'ementa': 'string',\n",
    "           'assunto': 'category',\n",
    "            'ementa_processada': 'string'\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando o tamanho individual das ementas apenas para fins de comparação entre elas\n",
    "tam_ementa_processada = []\n",
    "tam_ementa_tokenizada= []\n",
    "\n",
    "for x in df_limpo[\"ementa_processada\"]:\n",
    "    tam_ementa_processada.append(len(x))\n",
    "for x in df_limpo[\"ementa_tokenizada\"]:\n",
    "    tam_ementa_tokenizada.append(len(x))\n",
    "    \n",
    "df_limpo['tam_ementa_processada'] = tam_ementa_processada\n",
    "df_limpo['tam_ementa_tokenizada'] = tam_ementa_tokenizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculando o tamanho individual das ementas apenas para fins de comparação entre elas2\n",
    "tam_ementa = []\n",
    "tam_ementa_processada= []\n",
    "\n",
    "for x in df_limpo[\"ementa\"]:\n",
    "    tam_ementa.append(len(x))\n",
    "for x in df_limpo[\"ementa_processada\"]:\n",
    "    tam_ementa_processada.append(len(x))\n",
    "    \n",
    "df_limpo['tam_ementa'] = tam_ementa\n",
    "df_limpo['tam_ementa_processada'] = tam_ementa_processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando o pandas Profiling\n",
    "profile = ProfileReport(df_limpo, title='Analise Exploratória do Data Frame',html={'style':{'full_width':True}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando a primeira versão do DF Limpo\n",
    "df_limpo.to_csv('df_limpo.csv', sep = ';', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirmando duplicados\n",
    "print(\"Existem Duplicados? : \" +  str(any(df_limpo['ementa_processada'].duplicated())))\n",
    "print(\"Linhas duplicadas: \" +  str(4541 - df_limpo['ementa_processada'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo = df_limpo.drop_duplicates('ementa_processada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_limpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = df_limpo['tam_ementa'].corr(df_limpo['tam_ementa_processada'], method = 'pearson')\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "texto = ' '. join(df_limpo['ementa_processada'])\n",
    "wordCloud = WordCloud(max_font_size = 160, width = 1400, height = 700, colormap = 'viridis', background_color = 'white').generate(texto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordCloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo2 = df_limpo.drop_duplicates('ementa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_limpo2.shape)\n",
    "print(df_limpo2['ementa'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novas_stopwords = ['legais', 'dispositivos', 'aprovada', 'decreto', 'rfb', 'rgi','legal', 'dispositivo']\n",
    "for i in novas_stopwords:\n",
    "    stopwords.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pré-Processamento do texto2\n",
    "\n",
    "\n",
    "#primeiro tokeniza, para depois retirar as sopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens\"] = df[\"ementa\"].apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "#Removendo as stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return[token for token in tokens if token not in stopwords]\n",
    "corpus_stops = [' '.join(remove_stopwords(doc)) for doc in df['tokens']]\n",
    "\n",
    "#retirando acentos\n",
    "df['ementa_limpa']= corpus_stops\n",
    "ementa3_clean = [unicodedata.normalize('NFKD', s).encode('ISO-8859-1', 'ignore').decode('ISO-8859-1') for s in df['ementa_limpa']]\n",
    "\n",
    "#retirando todos os caracteres especiais ->[re.sub('[^\\w\\d\\s]',r'\n",
    "ementa_limpa2 = [re.sub(r'[^a-zA-z]',r' ', doc).lower() for doc in ementa3_clean ]\n",
    "df['ementa_processada'] = ementa_limpa2\n",
    "\n",
    "#Tokenizando novamente a lista limpa das stop words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df[\"tokens2\"] = df[\"ementa_processada\"].apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "#Realizando o stemming depois de retirada as stopwords\n",
    "def Stemming(sentence):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return phrase\n",
    "\n",
    "a = df[\"tokens2\"]\n",
    "ementa_processada2= []\n",
    "\n",
    "for i in a:\n",
    "    ementa_processada2.append(Stemming(i))\n",
    "\n",
    "#colocando com stemming no df\n",
    "df['ementa_processada2'] = ementa_processada2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destokenizando2\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "ementa_processada3 = []\n",
    "\n",
    "for i in ementa_processada2:\n",
    "    ementa_processada3.append(TreebankWordDetokenizer().detokenize(i))\n",
    "df['ementa_processada3'] = ementa_processada3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando o df_limpo2\n",
    "\n",
    "df_limpo = pd.DataFrame(data={'ementa': df['ementa'],\n",
    "                              'ementa_processada': df['ementa_processada3'],\n",
    "                              'assunto': df['assunto']})\n",
    "df_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirmando duplicados2\n",
    "print(\"Existem Duplicados? : \" +  str(any(df_limpo['ementa_processada'].duplicated())))\n",
    "print(\"Linhas duplicadas: \" +  str(4541 - df_limpo['ementa_processada'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retirando os duplicados2\n",
    "df_limpo = df_limpo.drop_duplicates('ementa_processada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_limpo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "texto = ' '. join(df_limpo['ementa_processada'])\n",
    "wordCloud = WordCloud(max_font_size = 160, width = 1400, height = 700, colormap = 'viridis', background_color = 'white').generate(texto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordCloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table3 = pd.pivot_table(df_limpo, values='ementa_processada', index=['assunto'],  aggfunc='count' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo = df_limpo.query('assunto != [\"NÃO ENCONTRADO\"]') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo = df_limpo.drop(columns =['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo = df_limpo.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus = df_limpo[\"ementa_processada\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf \n",
    "ngram = (1,2)\n",
    "vetorizador = TfidfVectorizer(ngram_range = ngram ,min_df = 2, max_df = .85, max_features = 5000)\n",
    "# Separando o texto da variável alvo\n",
    "list_corpus = df_limpo[\"ementa_processada\"].tolist()\n",
    "list_labels = df_limpo[\"assunto\"].tolist()\n",
    "\n",
    "#transformando o corpus em matriz esparsa \n",
    "matriz_esparsa_tfdif = vetorizador.fit_transform(list_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vetorizador.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_esparsadf = pd.DataFrame(matriz_esparsa_tfdif.toarray(), columns = tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocabulario = pd.DataFrame(vetorizador.vocabulary_, index = ['vocabulario']).T.sort_values(by='vocabulario', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_esparsadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_esparsadf.to_csv('matrizesparsa.csv',sep = ';' , encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limpo2b.to_csv('dflimpo.csv',sep = ';' , encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testando a redução de dimensionalidade com menor perda com base na variance ratio\n",
    "data = matriz_esparsa_tfdif\n",
    "explained =[]\n",
    "n_comp = [100,150,200,500,800,900,1000,1500,2000,2500]\n",
    "for x in n_comp:\n",
    "    svd = TruncatedSVD(n_components = x)\n",
    "    svd.fit(data)\n",
    "    explained.append(svd.explained_variance_ratio_.sum())\n",
    "    #lsa = NMF(n_components = x)\n",
    "    #lsa.fit(data)\n",
    "   # explained.append(sum(lsa.explained_variance_ratio))\n",
    "    print(\"Número de componentes = %r e número da variância = %r\"%(x,svd.explained_variance_ratio_.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfico Número de features x Valor variância explicada\n",
    "plt.plot(n_comp,explained)\n",
    "plt.xlabel(\"Número de Componentes\")\n",
    "plt.ylabel(\"Variância Explicada\")\n",
    "plt.title(\"Número de Componentes x Variância Explicada\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando redução de dimensionalidade\n",
    "\n",
    "t0 = time.time()\n",
    "svd = TruncatedSVD(900)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "corpus_transformed = lsa.fit_transform(matriz_esparsa_tfdif)\n",
    "print(\"  done in %.3fsec\" % (time.time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testando as dimensões reduzidas e a compatibiliade da matriz esparsa com as labels\n",
    "print(corpus_transformed.shape)\n",
    "print(len(list_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando os modelos para fazer o treinamento\n",
    "\n",
    "modelos =[]\n",
    "modelos.append(('Árvores de Decisão', DecisionTreeClassifier()))\n",
    "modelos.append(('Random Forest Classifier', RandomForestClassifier()))\n",
    "modelos.append(('Gaussian Naive Bayes',GaussianNB()))\n",
    "modelos.append(('Ridge Classifier', RidgeClassifier()))\n",
    "modelos.append(('Linear Discriminant Analysis', LinearDiscriminantAnalysis()))\n",
    "modelos.append(('Support Vector Machines', SVC()))\n",
    "modelos.append(('Regressão Logística Multinomial', LogisticRegression(multi_class = 'multinomial', solver ='saga')))\n",
    "modelos.append(('K Neighbors Classifier', KNeighborsClassifier()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando Dicionário para os modelos\n",
    "modelosDic= {'Árvores de Decisão': 'CART',\n",
    "            'Random Forest Classifier': 'RFC',\n",
    "            'Gaussian Naive Bayes': 'GNB',\n",
    "            'Ridge Classifier': 'RIDGE',\n",
    "            'Linear Discriminant Analysis': 'LDA',\n",
    "            'Support Vector Machines': 'SVM',\n",
    "            'Regressão Logística Multinomial': 'RLM',\n",
    "            'K Neighbors Classifier': 'KNN'}\n",
    "siglasmodelos = [modelosDic[x] for x in modelosDic]\n",
    "print(siglasmodelos)\n",
    "modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelosDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando lista para medidas de avaliação\n",
    "medidasaval = ['accuracy', 'precision_weighted','recall_weighted','f1_weighted']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando data frame para armazenar os resultados\n",
    "df_avaliacao = pd.DataFrame(columns = ['Medida', 'Algoritmo','Sigla', 'Média', \n",
    "                                       'Devio-Padrão','Coeficiente de variação', 'Valor mínimo', 'Valor Máximo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avaliacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation para avaliação de algoritmos\n",
    "\n",
    "t0 = time.time()\n",
    "for medaval in medidasaval:\n",
    "    resultados =[]\n",
    "    modeloslista = []\n",
    "\n",
    "    \n",
    "    for modeloNome, modelo in modelos:\n",
    "        metodo_crossvalidation = model_selection.KFold (n_splits= 10, shuffle = False, random_state = None)\n",
    "        scores = model_selection.cross_val_score(modelo, corpus_transformed, list_labels, cv= metodo_crossvalidation, \n",
    "                                                 scoring = medaval)\n",
    "        \n",
    "        resultados.append(scores)\n",
    "        modeloslista.append(modeloNome)\n",
    "        df_avaliacao.loc[len(df_avaliacao)]=[medaval,\n",
    "                                            modeloNome,\n",
    "                                            modelosDic[modeloNome],\n",
    "                                            scores.mean(),\n",
    "                                            scores.std(),\n",
    "                                            scores.std()/scores.mean(),\n",
    "                                            scores.min(),\n",
    "                                            scores.max()]\n",
    "        \n",
    "print(\"  done in %.3fsec\" % (time.time() - t0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizado o dataframe de avaliação\n",
    "df_avaliacao.sort_values(['Medida', 'Média'], ascending = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = [modelosDic[x] for x in modeloslista]\n",
    "imagem = plt.figure()\n",
    "imagem.suptitle('Comparação dos Algorítimos pela acurária')\n",
    "ax = imagem.add_subplot(111)\n",
    "plt.boxplot(resultados)\n",
    "ax.set_xticklabels(boxplot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in resultados:\n",
    "        print(i.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinando o classificador com o melhor desempenho\n",
    "x_train, x_test, y_train, y_test = train_test_split(corpus_transformed, list_labels, test_size = 0.2, random_state =0)\n",
    "classificador =  RidgeClassifier(random_state = None)\n",
    "classificador.fit(x_train, y_train)\n",
    "print(str(classificador.score(corpus_transformed, list_labels)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classificador.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construindo e visualizando a matriz de confusão\n",
    "a = confusion_matrix(y_test, pred)\n",
    "df_cm = pd.DataFrame(a, index = ['CLASSIFICAÇÃO DE MERCADORIAS',       \n",
    "'CONTRIBUIÇÃO DE INTERVENÇÃO NO DOMÍNIO ECONÔMICO',       \n",
    "'CONTRIBUIÇÕES SOCIAIS E PREVIDENCIÁRIAS',       \n",
    "'IMPOSTO SOBRE A IMPORTAÇÃO',       \n",
    "'IMPOSTO SOBRE A RENDA DE PESSOA FÍSICA',       \n",
    "'IMPOSTO SOBRE A RENDA DE PESSOA JURÍDICA',       \n",
    "'IMPOSTO SOBRE A RENDA RETIDO NA FONTE',       \n",
    "'IMPOSTO SOBRE PRODUTOS INDUSTRIALIZADOS',       \n",
    "'IOF',       \n",
    "'NORMAS DE ADMINISTRAÇÃO TRIBUTÁRIA',       \n",
    "'NORMAS GERAIS DE DIREITO TRIBUTÁRIO',       \n",
    "'OBRIGAÇÕES ACESSÓRIAS',       \n",
    "'REGIMES ADUANEIROS',       \n",
    "'SIMPLES NACIONAL'],\n",
    "              columns = [\n",
    "    'CLASSIFICAÇÃO DE MERCADORIAS',       \n",
    "'CONTRIBUIÇÃO DE INTERVENÇÃO NO DOMÍNIO ECONÔMICO',       \n",
    "'CONTRIBUIÇÕES SOCIAIS E PREVIDENCIÁRIAS',       \n",
    "'IMPOSTO SOBRE A IMPORTAÇÃO',       \n",
    "'IMPOSTO SOBRE A RENDA DE PESSOA FÍSICA',       \n",
    "'IMPOSTO SOBRE A RENDA DE PESSOA JURÍDICA',       \n",
    "'IMPOSTO SOBRE A RENDA RETIDO NA FONTE',       \n",
    "'IMPOSTO SOBRE PRODUTOS INDUSTRIALIZADOS',       \n",
    "'IOF',       \n",
    "'NORMAS DE ADMINISTRAÇÃO TRIBUTÁRIA',       \n",
    "'NORMAS GERAIS DE DIREITO TRIBUTÁRIO',       \n",
    "'OBRIGAÇÕES ACESSÓRIAS',       \n",
    "'REGIMES ADUANEIROS',       \n",
    "'SIMPLES NACIONAL'])\n",
    "plt.figure(figsize = (20,10))\n",
    "sn.heatmap(df_cm, annot=True,cmap=\"OrRd\", vmin=0, vmax=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relatório de Classificação\n",
    "print('\\nRelatório de classificação: \\n', classification_report(y_test, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
